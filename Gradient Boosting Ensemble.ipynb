{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, in this lesson, you will discover the gradient boosting ensemble.\n",
    "\n",
    "Gradient boosting is a framework for boosting ensemble algorithms and an extension to AdaBoost.\n",
    "\n",
    "It re-frames boosting as an additive model under a statistical framework and allows for the use of arbitrary loss functions to make it more flexible and loss penalties (shrinkage) to reduce overfitting.\n",
    "\n",
    "Gradient boosting also introduces ideas of bagging to the ensemble members, such as sampling of the training dataset rows and columns, referred to as stochastic gradient boosting.\n",
    "\n",
    "It is a very successful ensemble technique for structured or tabular data, although it can be slow to fit a model given that models are added sequentially. More efficient implementations have been developed, such as the popular extreme gradient boosting (XGBoost) and light gradient boosting machines (LightGBM).\n",
    "\n",
    "Gradient boosting is available in scikit-learn via the GradientBoostingClassifier and GradientBoostingRegressor classes, which use a decision tree as the base-model by default. You can specify the number of trees to create via the n_estimators argument and the learning rate that controls the contribution from each tree via the learning_rate argument that defaults to 0.1.\n",
    "\n",
    "The complete example of evaluating a gradient boosting ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.927 (0.100)\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a gradient boosting ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = GradientBoostingClassifier(n_estimators=50)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
